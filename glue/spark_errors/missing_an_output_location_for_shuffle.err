Spark Metadata Fetch Failed Exception: Missing an output location for shuffle

ERROR STACKTRACE will look like : 

2020-11-19 19:38:52,874 ERROR executionlogs:128 - g-e77a768b31735ca3fe3ee126fbf6d5ed62dca00d:2020-11-19 19:38:52,871 WARN [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logWarning(66)): Lost task 0.2 in stage 289.3 (TID 132629, 172.34.154.194, executor 1640): FetchFailed(null, shuffleId=41, mapId=-1, reduceId=0, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 41

Problem definition : 

MetadataFetchFailedException is thrown when a MapOutputTracker on an executor could not find requested shuffle map outputs for partitions in local cache and tried to fetch them remotely from the driver's MapOutputTracker

That could lead to few conclusions:

The driver's memory issues
The executors' memory issues
Executors being lost

Please review the logs looking for issues reported as "Executor lost" INFO messages and/or review web UI's Executors page and see how the executors work


RootCause :

For MetadataFetchFailedException, it usually happens when one executor suddenly being killed or terminated, but this executor has some shuffle output, then when another executor try to fetch metadata of this shuffle output, exception happens.

For FetchFailedException, it usually happens when one executor hosting some shuffle output is too busy or temporily dead. This could be caused by slow disk IO or network IO. It could be common when you have over 1000 executors.


1. Primarily due to repartition 
2. more memory to the worker node 

Resolution :

1) You could try to run with more partitions (do a repartition on your dataframe). Memory issues typically arise when one or more partitions contain more data than will fit in memory.

2) I'm noticing that you have not explicitly set spark.yarn.executor.memoryOverhead, so it will default to max(386, 0.10* executorMemory) which in your case will be 400MB. That sounds low to me. I would try to increase it to say 1GB (note that if you increase memoryOverhead to 1GB, you need to lower --executor-memory to 3GB)

3) Look in the log files on the failing nodes. You want to look for the text "Killing container". If you see the text "running beyond physical memory limits", increasing memoryOverhead will - in my experience - solve the problem.

4) --conf spark.blacklist.enabled=true - This can be set either at the job level using the --conf job argument --conf spark.blacklist.enabled=true or in the job script as a SparkConf before creating the SparkContext.

The blacklist feature will make sure that a task is not rescheduled on the failed executor more than once. This will prevent the race condition from happening.Details on spark executor blacklist - https://blog.cloudera.com/blacklisting-in-apache-spark/

5)  leaded to MetadataFetchFailedException that in map stage not reduce stage . just do df_rdd.repartition(nums) before reduceByKey()

6) It will probably help to troubleshoot if needed by the ST - Try using the job parameter: --enable-monitoring true

This will cause the driver to dump the thread stack if this occurs again and gain more visibility to the issue.Please note that this is different from the --enable-metrics flag. --enable-monitoring is an internal debug flag that is not documented.

7) You can enable spark debug level logs by doing glueContext.sparkContext.setLogLevel("DEBUG") for troubleshooting purpose.



